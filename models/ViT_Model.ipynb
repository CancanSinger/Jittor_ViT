{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22d8f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.path.abspath('.')\n",
    "\n",
    "project_root = current_dir\n",
    "\n",
    "project_root = os.path.dirname(project_root)\n",
    "\n",
    "if project_root in sys.path:\n",
    "    sys.path.remove(project_root)\n",
    "\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import jittor as jt\n",
    "from jittor import nn\n",
    "import math\n",
    "from config import Config\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35222505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Layers.layer_norm import LayerNorm\n",
    "from models.Layers.MLP import MLP_Layer\n",
    "from models.Layers.attention import MultiHeadSelfAttention\n",
    "from models.Layers.patch_embed import patch_embedding_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d74798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                embed_dim=config.EMBED_DIM,\n",
    "                num_heads=config.NUM_HEADS,\n",
    "                dropout_rate=config.DROPOUT,\n",
    "                hidden_dim=config.MLP_Hidden_Dim):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.Multi_Head_Self_Attention = MultiHeadSelfAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.MLP = MLP_Layer(\n",
    "            input_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=embed_dim,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "    def execute(self, x):\n",
    "        y = self.norm1(x)\n",
    "        y = self.Multi_Head_Self_Attention(y)\n",
    "        x = x + y\n",
    "        \n",
    "        z = self.norm2(x)\n",
    "        z = self.MLP(z)\n",
    "        x = x + z\n",
    "\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9914018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                img_size=config.IMG_SIZE,\n",
    "                patch_size=config.PATCH_SIZE,\n",
    "                in_channels=config.IN_CHANNELS,\n",
    "\n",
    "                embed_dim=config.EMBED_DIM,\n",
    "                depth=config.NUM_LAYERS,\n",
    "                num_heads=config.NUM_HEADS,\n",
    "                dropout_rate=config.DROPOUT,\n",
    "                hidden_dim=config.MLP_Hidden_Dim):\n",
    "        super(Visual_Transformer, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.patch_embed = patch_embedding_Layer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )   \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout_rate=dropout_rate,\n",
    "                hidden_dim=hidden_dim\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, config.NUM_CLASSES)\n",
    "        )\n",
    "    \n",
    "    def execute(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        class_token = x[:, 0]\n",
    "        logits = self.classifier(class_token)\n",
    "        \n",
    "        return logits\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe9ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
