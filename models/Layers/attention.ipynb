{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root determined as: /home/jittor/SCC_Model/ViT\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "from jittor import nn\n",
    "import math\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.path.abspath('.')\n",
    "project_root = current_dir\n",
    "\n",
    "for _ in range(2):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "print(f\"Project root determined as: {project_root}\")\n",
    "\n",
    "if project_root in sys.path:\n",
    "    sys.path.remove(project_root)\n",
    "\n",
    "sys.path.insert(0, project_root)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42831d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,embedded_dim,num_heads,dropout_rate=config.DROPOUT):\n",
    "        super(MultiHeadSelfAttention,self).__init__()\n",
    "        self.embedded_dim = embedded_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.every_head_dim = embedded_dim // num_heads\n",
    "        self.scale = math.sqrt(self.every_head_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.qkv_layer = nn.Linear(embedded_dim,embedded_dim*3)\n",
    "        self.out_layer = nn.Linear(embedded_dim,embedded_dim)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def execute(self,x:jt.Var,mask:jt.Var=None)->jt.Var:\n",
    "        B,N,C = x.shape\n",
    "        # [B,N,embedded_dim]->[B,N,3*embedded_dim]\n",
    "        qkv = self.qkv_layer(x)  \n",
    "        # [B,N,3*embedded_dim]->[B,N,3,num_heads,every_head_dim]\n",
    "        qkv = qkv.view(B,N,3,self.num_heads,self.every_head_dim)\n",
    "        #[B,N,3,num_heads,every_head_dim]-> 3* [B,N,num_heads,every_head_dim]\n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "        #[B,N,num_heads,every_head_dim]->[B,num_heads,N,every_head_dim]\n",
    "        q=q.permute(0,2,1,3)\n",
    "        k=k.permute(0,2,1,3)\n",
    "        v=v.permute(0,2,1,3)\n",
    "\n",
    "        #计算注意力得分，得分维度为[B, num_heads, N, N]\n",
    "        attention_scores = jt.matmul(q,k.transpose(0,1,3,2))/self.scale\n",
    "        #应用mask（如果有的话）\n",
    "        if mask is not None:\n",
    "            mask = mask.view(mask.shape[0], 1, 1, mask.shape[1])\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        #应用softmax\n",
    "        attention_scores = nn.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.attn_drop(attention_scores)\n",
    "        #计算注意力加权值\n",
    "        # [B,num_heads,N,N]*[B,num_heads,N,every_head_dim]->[B,num_heads,N,every_head_dim]\n",
    "        attention_output = jt.matmul(attention_scores, v)\n",
    "        # [B,num_heads,N,every_head_dim]->[B,N,embedded_dim]\n",
    "        attention_output = attention_output.permute(0,2,1,3).reshape(B,N,C)\n",
    "        #通过输出线性层\n",
    "        output = self.out_layer(attention_output)\n",
    "        #out的维度为[B,N,embedded_dim]\n",
    "        return output   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ac7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(4/23) used: 2.31s eta:   11s \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: [2,5,12,]\n",
      "输出形状: [2,5,12,]\n",
      "\n",
      "输入示例:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Operators(6/23) used: 3.32s eta: 9.41s 7/23) used: 4.33s eta: 9.89s 9/23) used: 5.33s eta: 8.29s 11/23) used: 6.34s eta: 6.91s 13/23) used: 7.34s eta: 5.65s 15/23) used: 8.35s eta: 4.45s 17/23) used: 9.35s eta:  3.3s 20/23) used: 10.4s eta: 1.55s 22/23) used: 11.4s eta: 0.517s 23/23) used: 12.4s eta:    0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jt.Var([[ 3.1880727e-01 -9.3096596e-01 -3.5460648e-01  2.0113297e+00\n",
      "         -4.7587970e-01 -3.5760027e-01  2.7498421e-01  6.1297852e-01\n",
      "          2.5876823e-01 -9.6165931e-01  1.6200861e+00  6.6917324e-01]\n",
      "        [-7.3587066e-01  1.0585311e-02  3.3337894e-01  7.5165236e-01\n",
      "          4.5378679e-01  1.5888499e+00  9.6721357e-01  4.2898846e-01\n",
      "         -9.3403971e-03  4.7813603e-01 -1.0119213e+00 -2.6517095e-02]\n",
      "        [ 8.2607633e-01 -3.2819703e-01 -5.7165623e-01 -2.2431624e+00\n",
      "          2.0486769e-01  3.3215874e-01 -2.8591242e-01 -1.1700377e+00\n",
      "          7.4926531e-01  8.9712590e-03 -4.5629728e-01 -4.3678969e-01]\n",
      "        [ 1.6747835e-01 -1.8246700e+00 -7.3218602e-01 -1.6868219e+00\n",
      "         -9.9124110e-01  9.7814798e-01  1.0903381e+00 -7.1771753e-01\n",
      "          2.3294131e-01  1.0095409e+00  2.8932983e-01 -2.5322038e-01]\n",
      "        [ 7.7399069e-01  1.4942355e+00 -9.5671952e-01  4.8368138e-01\n",
      "          1.5254791e-01  9.6693464e-02  7.6289165e-01  2.0495662e-01\n",
      "         -9.7108068e-04  5.8429652e-01 -2.0008759e+00  7.3031527e-01]], dtype=float32)\n",
      "\n",
      "输出示例:\n",
      "jt.Var([[-0.05792803 -0.13364106 -0.00496018 -0.18145207  0.04587759  0.3517208\n",
      "          0.2206183   0.33261582  0.13709465  0.27297866 -0.0364912   0.3489444 ]\n",
      "        [-0.04287435 -0.13200775  0.01241153 -0.20956351  0.025655    0.367497\n",
      "          0.17645618  0.31987816  0.17801592  0.3060148  -0.05742629  0.3016053 ]\n",
      "        [-0.06753121 -0.15524085 -0.04314633 -0.11311448  0.01754799  0.31541106\n",
      "          0.1962121   0.33087796  0.16685909  0.23420231 -0.02185953  0.3915251 ]\n",
      "        [-0.09189056 -0.16104718 -0.09415602 -0.11303687  0.05997071  0.27347568\n",
      "          0.17263243  0.32906985  0.13967407  0.23540524  0.04309446  0.4247282 ]\n",
      "        [-0.03818563 -0.1399944   0.04028882 -0.15972623 -0.02356228  0.39059418\n",
      "          0.20250735  0.32790807  0.21306477  0.2648353  -0.10881472  0.3181849 ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试参数\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "embedded_dim = 12\n",
    "num_heads = 4\n",
    "\n",
    "# 创建测试数据\n",
    "x = jt.randn(batch_size, seq_length, embedded_dim)\n",
    "mask = jt.ones(batch_size, seq_length)  # 改回正确的形状 [batch_size, seq_length]\n",
    "\n",
    "# 创建多头注意力层\n",
    "attention = MultiHeadSelfAttention(embedded_dim, num_heads)\n",
    "\n",
    "# 运行测试\n",
    "output = attention(x, mask)\n",
    "print(\"输入形状:\", x.shape)\n",
    "print(\"输出形状:\", output.shape)\n",
    "print(\"\\n输入示例:\")\n",
    "print(x[0])  # 打印第一个batch的数据\n",
    "print(\"\\n输出示例:\")\n",
    "print(output[0])  # 打印第一个batch的输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9859970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
